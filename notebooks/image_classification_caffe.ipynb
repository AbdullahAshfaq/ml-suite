{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Caffe\n",
    "\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Caffe model for FPGA acceleration using Xilinx MLSuite:  \n",
    "1. **Quantize the model** - The quantizer will generate scaling parameters for quantizing floats INT8. This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve more parallelization at lower power. \n",
    "2. **Compile the Model** - In this step, the network Graph (prototxt) and the Weights (caffemodel) are compiled, the compiler \n",
    "3. **Subgraph Cutting** - In this step, the original graph is cut, and a custom FPGA accelerated python layer is inserted to be used for Inference. \n",
    "4. **Classification** - In this step, the caffe model and the prototxt from the previous step are run on the FPGA to perform inference on an input image.\n",
    "  \n",
    "For command line versions see: examples/caffe/  \n",
    "  \n",
    "## Prerequisite Files\n",
    "1. **Model files** - This notebook requires that model files are located in  \n",
    "  `/opt/models/caffe`\n",
    "2. **Image files** - This notebook requires ilsvrc2012 image files are downloaded in  \n",
    "  `HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/`\n",
    "  \n",
    "## Setup (Before Running Notebook)\n",
    "```\n",
    "cd /opt/ml-suite/examples/caffe\n",
    "python -m ck pull repo:ck-env\n",
    "python -m ck install package:imagenet-2012-val-min\n",
    "python -m ck install package:imagenet-2012-aux\n",
    "head -n 500 $HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/val.txt > $HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val_map.txt\n",
    "python resize.py $HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min 256 256\n",
    "source $MLSUITE_ROOT/overlaybins/setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from IPython.display import Image as display\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "from caffe import Classifier, io\n",
    "from caffe.proto import caffe_pb2\n",
    "from caffe.draw import draw_net_to_file\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Environment Variables (\"source overlaybins/setup.sh\")\n",
    "HOME = os.getenv(\"HOME\",\"/home/mluser\")\n",
    "MLSUITE_ROOT = os.getenv(\"MLSUITE_ROOT\",os.getcwd()+\"/..\")\n",
    "MLSUITE_PLATFORM = os.getenv(\"MLSUITE_PLATFORM\",\"alveo-u200\")\n",
    "XCLBIN = MLSUITE_ROOT+\"/overlaybins/\"+MLSUITE_PLATFORM+\"/overlay_4.xclbin\"\n",
    "\n",
    "print(\"Running w/ HOME: %s\" % HOME)\n",
    "print(\"Running w/ MLSUITE_ROOT: %s\" % MLSUITE_ROOT)\n",
    "print(\"Running w/ XCLBIN: %s\" % XCLBIN)\n",
    "print(\"Running w/ MLSUITE_PLATFORM: %s\" % MLSUITE_PLATFORM)\n",
    "\n",
    "# moving to MLSUITE_ROOT directory\n",
    "os.chdir(MLSUITE_ROOT)\n",
    "\n",
    "# Bring in ml-suite Quantizer, Compiler, SubGraph Cutter\n",
    "from decent import CaffeFrontend as xfdnnQuantizer\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_caffe  import CaffeFrontend as xfdnnCompiler\n",
    "from xfdnn.rt.scripts.framework.caffe.xfdnn_subgraph import CaffeCutter as xfdnnCutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Choose a model\n",
    "Choose a model using the drop down, or select custom, and enter your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(MODEL=[\"bvlc_googlenet\",\"inception_v2\",\"inception_v3\",\"inception_v4\",\\\n",
    "                    \"resnet50_v1\",\"resnet50_v2\",\"squeezenet\",\"vgg16\",\"custom\"])\n",
    "def selectModel(MODEL):\n",
    "    global prototxt\n",
    "    global caffemodel\n",
    "    model_root = \"/opt/models/caffe/\"\n",
    "    if MODEL == \"custom\":\n",
    "        prototxt = None\n",
    "        caffemodel = None\n",
    "    else:\n",
    "        prototxt = model_root + MODEL + \"/\" + MODEL + \"_train_val.prototxt\"\n",
    "        caffemodel = model_root + MODEL + \"/\" + MODEL + \".caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not prototxt:\n",
    "    @interact(PROTOTXT=\"Provide the path to your prototxt\")\n",
    "    def selectPrototxt(PROTOTXT):\n",
    "        global prototxt\n",
    "        prototxt = PROTOTXT\n",
    "    @interact(CAFFEMODEL=\"Provide the path to your caffemodel\")\n",
    "    def selectCaffemodel(CAFFEMODEL):\n",
    "        global caffemodel\n",
    "        caffemodel = CAFFEMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running with prototxt:   %s\" % prototxt)\n",
    "print(\"Running with caffemodel: %s\" % caffemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run the Quantizer\n",
    "\n",
    "Here, we will quantize the model. The inputs are model prototxt, model weights, number of test iterations and calibration iterations. The output is quantized prototxt, weights, and quantize_info.txt and will be generated in the quantize_results/ directory.\n",
    "\n",
    "The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT8\n",
    "This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve accelerated inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize(prototxt,caffemodel,calib_iter=1):\n",
    "    \n",
    "    quantizer = xfdnnQuantizer(\n",
    "        model=prototxt,\n",
    "        weights=caffemodel,\n",
    "        calib_iter=calib_iter,\n",
    "    )\n",
    "    \n",
    "    quantizer.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantize(prototxt,caffemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Compiler\n",
    "\n",
    "The compiler takes in the quantizer outputs from the previous step (prototxt, weights, quantize_info) and outputs a compiler.json and quantizer.json.\n",
    "\n",
    "* A Network Graph (prototxt) and a Weights Blob (caffemodel) are compiled\n",
    "* The network is optimized\n",
    "* FPGA Instructions are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard compiler arguments - PLEASE DONT TOUCH\n",
    "def Getopts():\n",
    "    return {\n",
    "            \"bytesperpixels\":1,\n",
    "            \"dsp\":96,\n",
    "            \"memory\":9,\n",
    "            \"ddr\":256,\n",
    "            \"cpulayermustgo\":True,\n",
    "            \"forceweightsfullyconnected\":True,\n",
    "            \"mixmemorystrategy\":True,\n",
    "            \"pipelineconvmaxpool\":True,\n",
    "            \"usedeephi\":True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile(prototxt=\"quantize_results/deploy.prototxt\",\\\n",
    "            caffemodel=\"quantize_results/deploy.caffemodel\",\\\n",
    "            quantize_info=\"quantize_results/quantize_info.txt\"):\n",
    "    \n",
    "    compiler = xfdnnCompiler(\n",
    "        networkfile=prototxt,\n",
    "        weights=caffemodel,\n",
    "        quant_cfgfile=quantize_info,\n",
    "        generatefile=\"work/compiler\",\n",
    "        quantz=\"work/quantizer\",\n",
    "        **Getopts()\n",
    "    )\n",
    "    \n",
    "    compiler.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Subgraph Cutter\n",
    "\n",
    "The subgraph cutter creates a custom python layer to be accelerated on the FPGA. The inputs are compiler.json, quantizer.json and model weights from the compiler step, as well as the FPGA xclbin. This outputs a cut prototxt file with FPGA references, to be used for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(prototxt):\n",
    "    \n",
    "    cutter = xfdnnCutter(\n",
    "        inproto=\"quantize_results/deploy.prototxt\",\n",
    "        trainproto=prototxt,\n",
    "        outproto=\"xfdnn_auto_cut_deploy.prototxt\",\n",
    "        outtrainproto=\"xfdnn_auto_cut_train_val.prototxt\",\n",
    "        cutAfter=\"data\",\n",
    "        xclbin=XCLBIN,\n",
    "        netcfg=\"work/compiler.json\",\n",
    "        quantizecfg=\"work/quantizer.json\",\n",
    "        weights=\"work/deploy.caffemodel_data.h5\",\n",
    "        profile=True\n",
    "    )\n",
    "    \n",
    "    cutter.cut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cut(prototxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize the new graph with the FPGA subgraph\n",
    "net = caffe_pb2.NetParameter()\n",
    "text_format.Merge(open(\"xfdnn_auto_cut_deploy.prototxt\").read(), net)\n",
    "draw_net_to_file(net,\"xfdnn_auto_cut_deploy.png\")\n",
    "display(\"xfdnn_auto_cut_deploy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Inference \n",
    "\n",
    "The inputs are the FPGA prototxt file, caffemodel weights, a test image, and the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify(prototxt,caffemodel,image,labels):\n",
    "    classifier = Classifier(prototxt,caffemodel,\n",
    "        mean=np.array([104,117,123]),\n",
    "        raw_scale=255, channel_swap=[2,1,0])\n",
    "\n",
    "    predictions = classifier.predict([io.load_image(image)]).flatten()\n",
    "    labels = np.loadtxt(labels, str, delimiter='\\t')\n",
    "    top_k = predictions.argsort()[-1:-6:-1]\n",
    "    for l,p in zip(labels[top_k],predictions[top_k]):\n",
    "        print (l,\" : \",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image to run, display it for reference\n",
    "image = HOME+\"/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/ILSVRC2012_val_00000002.JPEG\"\n",
    "display(filename=image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classify(\"xfdnn_auto_cut_deploy.prototxt\",caffemodel,image,HOME+\"/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/synset_words.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook demonstrates how to target Xilinx FPGAs for inference using Caffe.  \n",
    "The custom Python layer is defined in /opt/ml-suite/xfdnn/rt/scripts/framework/caffe/CaffeXFDNN.py  \n",
    "When the time comes to take your application to production please look at examples in /opt/ml-suite/examples/deployment_modes/  \n",
    "Highest performance is acheived by creating multiprocess pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
