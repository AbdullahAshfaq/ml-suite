[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=64
subdivisions=8

width=224
height=224
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 500200
policy=steps
steps=400000,450000
scales=.1,.1

# input:  224x224x3
# output: 224x224x32
# 3,3,32,1
# Layer 0
[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=32
size=3
stride=1
pad=1
activation=relu

# input:  224x224x32
# output: 112x112x32
[maxpool]
size=2
stride=2

# input:  112x112x32
# output: 112x112x64
# 32,3,64,1
# Layer 1
[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=64
size=3
stride=1
pad=1
activation=relu

# input:  112x112x64
# output: 56x56x64
[maxpool]
size=2
stride=2

# input:  56x56x64
# output: 56x56x128
# 64,3,128,1
# Layer 2
[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=128
size=3
stride=1
pad=1
activation=relu

# input:  56x56x128
# output: 56x56x64
# 64,3,128,1
# Layer 2
[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=64
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=128
size=3
stride=1
pad=1
activation=relu

[maxpool]
size=2
stride=2

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=256
size=3
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=128
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=256
size=3
stride=1
pad=1
activation=relu

[maxpool]
size=2
stride=2

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=512
size=3
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=256
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=512
size=3
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=256
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=512
size=3
stride=1
pad=1
activation=relu

[maxpool]
size=2
stride=2

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=1024
size=3
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=512
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=1024
size=3
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=512
size=1
stride=1
pad=1
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
filters=1024
size=3
stride=1
pad=1
activation=relu


#######

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
size=3
stride=1
pad=1
filters=1024
activation=relu

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
size=3
stride=1
pad=1
filters=1024
activation=relu

[route]
layers=-9

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
size=1
stride=1
pad=1
filters=64
activation=relu

#[reorg]
#stride=2
[maxpool]
size=2
stride=2

[route]
layers=-1,-4

[convolutional]
#batch_normalize=1 # batch norm merged into weights by xlnx
size=3
stride=1
pad=1
filters=1024
activation=relu

[convolutional]
size=1
stride=1
pad=1
filters=70
activation=linear


[region]
anchors =  0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828
bias_match=1
classes=9
coords=4
num=5
softmax=1
jitter=.3
rescore=1

object_scale=5
noobject_scale=1
class_scale=1
coord_scale=1

absolute=1
thresh = .6
random=1

truth_boxes_to_keep=1
