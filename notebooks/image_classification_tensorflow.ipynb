{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Tensorflow\n",
    "\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Tensorflow model for FPGA acceleration using Xilinx MLSuite:  \n",
    "1. **Quantize the model** - The quantizer will generate scaling parameters for quantizing floats INT8. This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve more parallelization at lower power. \n",
    "2. **Subgraph Cutting and Compilation** - In this step, the original graph is cut, compiled and a custom FPGA accelerated python layer is inserted to be used for Inference. \n",
    "4. **Classification** - In this step, the modified Tensorflow model from the previous step are run on the FPGA to perform inference on an input image.\n",
    "    \n",
    "## Prerequisite Files \n",
    "1. **Model files** - This notebook requires that model files are located in  \n",
    "  `/opt/models/tensorflow`\n",
    "2. **Image files** - This notebook requires ilsvrc2012 image files are downloaded in  \n",
    "  `HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/`\n",
    "  \n",
    "## Setup (Before Running Notebook)\n",
    "This notebook should be run inside a tensorflow docker container.\n",
    "\n",
    "Download the models to \"/opt/models/tensorflow\":\n",
    "```\n",
    "$ python /opt/ml-suite/examples/tensorflow/getModels.py\n",
    "```\n",
    "\n",
    "Download 500 calibration images into \"~/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/\":\n",
    "```\n",
    "$ python -m ck pull repo:ck-env\n",
    "$ python -m ck install package:imagenet-2012-val-min\n",
    "$ python -m ck install package:imagenet-2012-aux\n",
    "$ head -n 500 ~/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/val.txt > ~/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.display import Image as display\n",
    "from ipywidgets import interact\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Environment Variables (obtained by running \"source overlaybins/setup.sh\")\n",
    "HOME             = os.getenv('HOME','/home/mluser/')\n",
    "MLSUITE_ROOT     = os.getenv('MLSUITE_ROOT',os.getcwd()+'/../')\n",
    "MLSUITE_PLATFORM = os.getenv('MLSUITE_PLATFORM','alveo-u200')\n",
    "XCLBIN           = MLSUITE_ROOT + '/overlaybins/' + MLSUITE_PLATFORM + '/overlay_4.xclbin'\n",
    "\n",
    "MODELDIR   = \"/opt/models/tensorflow/\"\n",
    "IMAGEDIR   = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/\"\n",
    "IMAGELIST  = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val.txt\"\n",
    "LABELSLIST = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/synset_words.txt\"\n",
    "\n",
    "print(\"Running w/ HOME: %s\" % HOME)\n",
    "print(\"Running w/ MLSUITE_ROOT: %s\" % MLSUITE_ROOT)\n",
    "print(\"Running w/ XCLBIN: %s\" % XCLBIN)\n",
    "print(\"Running w/ MLSUITE_PLATFORM: %s\" % MLSUITE_PLATFORM)\n",
    "\n",
    "# moving to MLSUITE_ROOT directory\n",
    "os.chdir(MLSUITE_ROOT)\n",
    "\n",
    "# Bring in ml-suite Quantizer, Compiler, and Partitioner\n",
    "from xfdnn.rt.xdnn_rt_tf import TFxdnnRT as xdnnRT\n",
    "from xfdnn.rt.xdnn_util import make_list\n",
    "from xfdnn.rt.xdnn_io import default_xdnn_arg_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Choose a model\n",
    "Choose a model using the drop down, or select custom, and enter your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(MODEL=['resnet50','inception_v1','custom'])\n",
    "def selectModel(MODEL):\n",
    "    global protoBuffer, quantInfo, inputNode, outputNode, inputShape, means\n",
    "\n",
    "    default_protoBuffer = {'resnet50': 'resnet50_baseline.pb', 'inception_v1': 'inception_v1_baseline.pb', 'pedestrian_attribute': 'pedestrian_attributes_recognition_quantizations.pb'}\n",
    "    default_quantInfo   = {'resnet50': 'fix_info_resnet50_baseline.txt', 'inception_v1': 'fix_info_inception_v1_baseline.txt', 'pedestrian_attribute': 'fix_info_pedestrian_attributes_recognition_quantizations.txt'}\n",
    "    default_inputNode   = {'resnet50': 'data', 'inception_v1': 'data', 'pedestrian_attribute': 'data'}\n",
    "    default_outputNode  = {'resnet50': 'prob', 'inception_v1': 'loss3_loss3', 'pedestrian_attribute': 'pred_upper,pred_lower,pred_gender,pred_hat,pred_bag,pred_handbag,pred_backpack'}\n",
    "    default_inputShape  = {'resnet50': '224,224', 'inception_v1': '224,224', 'pedestrian_attribute': '224,128'}\n",
    "    default_means       = {'resnet50': '104,107,123', 'inception_v1': '104,107,123', 'pedestrian_attribute': '104,107,123'}\n",
    "\n",
    "    if MODEL == \"custom\":\n",
    "        protoBuffer = None\n",
    "        quantInfo   = None\n",
    "        inputNode   = None\n",
    "        outputNode  = None\n",
    "        inputShape  = None\n",
    "        means       = None\n",
    "    else:\n",
    "        protoBuffer = MODELDIR + default_protoBuffer[MODEL]\n",
    "        quantInfo   = MODELDIR + default_quantInfo[MODEL]\n",
    "        inputNode   = default_inputNode[MODEL]\n",
    "        outputNode  = default_outputNode[MODEL]\n",
    "        inputShape  = default_inputShape[MODEL]\n",
    "        means       = default_means[MODEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not protoBuffer:\n",
    "    @interact(PROTOBUFFER=\"Provide the path to your protobuffer\")\n",
    "    def selectPrototxt(PROTOBUFFER):\n",
    "        global protoBuffer\n",
    "        protoBuffer = PROTOBUFFER\n",
    "\n",
    "if not quantInfo:\n",
    "    @interact(QUANTINFO=\"Provide the path to your quantization file\")\n",
    "    def selectCaffemodel(QUANTINFO):\n",
    "        global quantInfo\n",
    "        quantInfo = QUANTINFO\n",
    "\n",
    "if not inputNode:\n",
    "    @interact(INPUTNODE=\"Provide the input node(s) (comma separated string with no spaces)\")\n",
    "    def selectCaffemodel(INPUTNODE):\n",
    "        global inputNode\n",
    "        inputNode = INPUTNODE\n",
    "\n",
    "if not outputNode:\n",
    "    @interact(OUTPUTNODE=\"Provide the output node(s) (comma separated string with no spaces)\")\n",
    "    def selectCaffemodel(INPUTNODE):\n",
    "        global outputNode\n",
    "        outputNode = OUTPUTNODE\n",
    "\n",
    "if not inputShape:\n",
    "    @interact(INPUTSHAPE=\"Provide the input shapes (comma separated string with no spaces)\")\n",
    "    def selectCaffemodel(INPUTSHAPE):\n",
    "        global inputShape\n",
    "        inputShape = INPUTSHAPE\n",
    "\n",
    "if not means:\n",
    "    @interact(MEANS=\"Provide the means (comma separated string with no spaces)\")\n",
    "    def selectCaffemodel(MEANS):\n",
    "        global means\n",
    "        means = MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running with protoBuffer:   %s\" % protoBuffer)\n",
    "print(\"Running with quantInfo:     %s\" % quantInfo)\n",
    "print(\"Running with inputNode:     %s\" % inputNode)\n",
    "print(\"Running with outputNode:    %s\" % outputNode)\n",
    "print(\"Running with inputShape:    %s\" % inputShape)\n",
    "print(\"Running with means:         %s\" % means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run the Quantizer (TBD)\n",
    "\n",
    "Inspect the model to gather its input and output node(s), and input nodes' shape.  Next, quantize the model using graph and sample data parameters.  This quantization process produces two protobuf files containing the quantization information.  Finally, extract the quantization information using the compiler.  The end result is a txt file holding the scaling parameters for quantizing floats to INT8.  This is required because FPGAs will take advantage of Fixed Point Precision to achieve accelerated inference.  Remember to conda activate decent before beginning this section and conda deactivate to return to the ml-suite conda environment before moving on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_batch_size = 1\n",
    "\n",
    "def calib_input(iter):\n",
    "  images = []\n",
    "  line = open(IMAGELIST).readlines()\n",
    "  for index in range(0, calib_batch_size):\n",
    "    curline = line[iter * calib_batch_size + index].strip()\n",
    "    [calib_image_name, calib_label_id] = curline.split(' ')\n",
    "\n",
    "    width_out = 128\n",
    "    height_out = 224\n",
    "\n",
    "    image = cv2.imread(IMAGEDIR + calib_image_name)\n",
    "    width_in = image.shape[1]\n",
    "    height_in = image.shape[0]\n",
    "\n",
    "    scale = max(float(height_out) / height_in, float(width_out) / width_in)\n",
    "\n",
    "    image = cv2.resize(image, (round(scale * width_in), round(s * height_in)))\n",
    "    width_in = image.shape[1]\n",
    "    height_in = image.shape[0]\n",
    "\n",
    "    width_begin = round(0.5 * (width_in - width_out))\n",
    "    height_begin = round(0.5 * (height_in - height_out))\n",
    "    width_end = width_begin + width_out\n",
    "    height_end = height_begin + height_out\n",
    "    image = image[width_begin:width_end, height_begin:height_end]\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    means = (123, 117, 104)\n",
    "    image -= means\n",
    "\n",
    "    images.append(image)\n",
    "  return {\"data\": images}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!decent_q inspect --input_frozen_graph $protoBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!decent_q quantize \\\n",
    "    --input_frozen_graph $protoBuffer \\\n",
    "    --input_nodes $inputNode \\\n",
    "    --output_nodes $outputNode \\\n",
    "    --input_shapes ?,$inputShape,3 \\\n",
    "    --input_fn default \\\n",
    "    --method 1 \\\n",
    "    --calib_iter 1 \\\n",
    "    --batch_size 10 \\\n",
    "    --output_dir $MODELDIR \\\n",
    "    --image_dir $IMAGEDIR \\\n",
    "    --image_list $IMAGELIST \\\n",
    "    --means $means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m xfdnn.tools.compile.bin.xfdnn_compiler_tensorflow \\\n",
    "    -n $MODELDIR/deploy_model.pb \\\n",
    "    -m 9 \\\n",
    "    --dsp 96 \\\n",
    "    -g $quantInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Partitioner and Compiler\n",
    "\n",
    "The partitioner takes in the model protoBuffer, and pre-computer quantization parameters and compiles the porttion of the network specified from starnode(s) to finalnode(s) for FPGA acceleration.\n",
    "\n",
    "In case startnode and/or finalnode is not specified (i.e., an empty list) the corresponding endnode is infered from the protoBuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(startnode=inputNode, finalnode=outputNode):\n",
    "    return {\n",
    "        ### Some standard partitioner arguments [EDITABLE]\n",
    "        'startnode':            startnode,\n",
    "        'finalnode':            finalnode,\n",
    "        \n",
    "        ### Some standard compiler arguments [PLEASE DONT TOUCH]\n",
    "        'dsp':                  96,\n",
    "        'memory':               9,\n",
    "        'bytesperpixels':       1,\n",
    "        'ddr':                  256,\n",
    "        'data_format':          'NHWC',\n",
    "        'mixmemorystrategy':    True,\n",
    "        'noreplication':        True,\n",
    "        'xdnnv3':               True,\n",
    "        'usedeephi':            True,\n",
    "        'quantz':               ''  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load default arguments\n",
    "FLAGS, unparsed = default_xdnn_arg_parser().parse_known_args([])\n",
    "\n",
    "### Partition and compile\n",
    "rt = xdnnRT(FLAGS,\n",
    "            networkfile=protoBuffer,\n",
    "            quant_cfgfile=quantInfo,\n",
    "            xclbin=XCLBIN,\n",
    "            device='FPGA',\n",
    "            **get_args(inputNode, outputNode)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Inference \n",
    "\n",
    "If the model in protoBuffer includes all the pre and post processings, the inference can be done simply by passing the input_data as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing function\n",
    "def preprocess(image):\n",
    "    input_height, input_width = 224, 224\n",
    "\n",
    "    ## Image preprocessing using numpy\n",
    "    img  = cv2.imread(image).astype(np.float32)\n",
    "    img -= np.array(make_list(means)).reshape(-1,3).astype(np.float32)\n",
    "    img  = cv2.resize(img, (input_width, input_height))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose image to run, display it for reference\n",
    "image  = IMAGEDIR + \"ILSVRC2012_val_00000003.JPEG\"\n",
    "\n",
    "display(filename=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accelerated execution\n",
    "\n",
    "## load the accelerated graph\n",
    "graph = rt.load_partitioned_graph()\n",
    "\n",
    "## run the tensorflow graph as usual (additional operations can be added to the graph)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    input_tensor  = graph.get_operation_by_name(inputNode).outputs[0]\n",
    "    output_tensor = graph.get_operation_by_name(outputNode).outputs[0]\n",
    "    \n",
    "    predictions = sess.run(output_tensor, feed_dict={input_tensor: [preprocess(image)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt(LABELSLIST, str, delimiter='\\t')\n",
    "top_k = predictions[0].argsort()[:-6:-1]\n",
    "\n",
    "for l,p in zip(labels[top_k], predictions[0][top_k]):\n",
    "    print (l,\" : \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook demonstrates how to target Xilinx FPGAs for inference using Caffe.  \n",
    "The custom Python layer is defined in /opt/ml-suite/xfdnn/rt/scripts/framework/caffe/CaffeXFDNN.py  \n",
    "When the time comes to take your application to production please look at examples in /opt/ml-suite/examples/deployment_modes/  \n",
    "Highest performance is acheived by creating multiprocess pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
