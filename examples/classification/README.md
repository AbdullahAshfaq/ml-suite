# Image Classification with Python APIs

## Introduction
This directory provides examples of how to deploy Deep CNNs on FPGAs using Xilinx Python APIs.

All examples provided within this directory exercise precompiled models whose components are stored in the local ./data directory.  

A "compiled model" consists of low level HW instructions, and quantization parameters. 

**Compiler Outputs**:  JSON FILE (HW Instructions *_56.json or *_28.json), model_data directory with preprocessed floating point weights.  
**Quantizer Outputs**: JSON FILE containing scaling factors for each layer in the corresponding network. (*_8b.json or *_16b.json)  

Important Notes:
 - The final layers of the network (Fully connected, Softmax) are run on the CPU, as those layers are not supported by the FPGA
 - The batch_classify example will require you to download the imagnet validation set, and place the images [here](../../models/data/ilsvrc12/ilsvrc12_img_val/replace_this_file_with_dataset.md)  
 - Amazon AWS EC2 F1 requires root privileges to load the FPGA, use the documented workaround

The following three examples of applications using the Python xfDNN API are provided:

1. A **Test Classification** example that demonstrates how to run inference on a single image "dog.jpg"
2. A **Batch Classifcation** example that streams images from disk through the FPGA for classification.
3. A **Multi-Process** example that shows different DNNs running on different processing elements on the FPGA.  

## Running the Examples  

To run any of the three examples, use the provided bash run.sh script. 

1. Start Anaconda.
    ```sh
    $ conda activate ml-suite
    ```

2. Navigate to the `ml-suite/examples/classification` dir.
    ```sh
    $ cd ml-suite/examples/classification
    ```

3. Familiarize yourself with the script usage by:  
  `./run.sh -h`  
  The key parameters are:
    - -p `platform` Valid values are `alveo-u200`, `alveo-u250`, `aws`, `nimbix`, `1525`, '1525-ml`
    - -t `test` - Valid values are `test_classify` or `batch_classify` or `multinet`
    - -k `kernel config` - Valid values are `med` or `large` - Used to select overlaybins
    - -b `quantization precision` - Valid values are `16` or `8` - corresponding to INT16 or INT8  

## Example Invocations
1. Single Image Classification on AWS, with medium kernels:
    ```sh
    $ ./run.sh -p aws -t test_classify -k med -b 16
    ```

5. Streaming Image Classification on alveo-u200 with large kernels:
    ```sh
    $ ./run.sh -p alveo-u200 -t batch_classify -k large -b 8
    ```
6. Multinet Image Classification on Nimbix (Currently, for Multinet only the med size kernel and 16b precision are supported)
    ```sh
    ./run.sh -p nimbix -t multinet -k med -b 16
    ```

## Example Script Switches
Take a look at the following scripts to understand the examples:
* run.sh
* test_classify.py
* batch_classify.py
* test_classify_async_multinet.py  

The scripts use the arg parser defined in [xdnn_io.py](../../xfdnn/rt/xdnn_io.py)

- `--xclbin` 		- Defines which FPGA binary overlay to use. The available binaries are stored in [overlaybins](../../overlaybins)
- `--netcfg` 	    - FPGA instructions generated by the Compiler for the network being ran
- `--quantcfg`      - Path to json file to use for quantization (The json file contains scaling params)
- `--fpgaoutsz`	    - Flattened size of the final activation computed by FPGA (The FPGA will not do FC layers or Softmax)
- `--firstfpgalayer` - Name of first FPGA layer i.e. conv0 (required by quantization code)
- `--datadir`		- Path to data files to run for the network (weights)
- `--labels`		- Path to text file containing line seperated labels
- `--xlnxlib`		- Path to xfDNN library file (Middleware)
- `--useblas`		- Use BLAS-optimized functions
- `--golden`		- Path to text file containing line seperated correct labels
- `--imagedir`	    - Directory with image files to classify (Only applicable to batch_classify)
- `--jsoncfg`       - Path to json file used to define seperate networks (Only applicable to multinet)

For Multinet deployments, the different models/networks are set in the `--jsoncfg` file. For the Multinet example given above, see how to set the arguments here [multinet.json][]

## Example Output From Single Image Classification

  ```sh
  $ ./run.sh -p 1525 -t test_classify -k med -b 16
  =============== pyXDNN =============================
  [XBLAS] # kernels: 1
  Linux:4.4.0-121-generic:#145-Ubuntu SMP Fri Apr 13 13:47:23 UTC 2018:x86_64
  Distribution: Ubuntu 16.04.2 LTS
  GLIBC: 2.23
  ---

  ---
  CL_PLATFORM_VENDOR Xilinx
  CL_PLATFORM_NAME Xilinx
  CL_DEVICE_0: 0x175a1d0
  CL_DEVICES_FOUND 1, using 0
  loading /home/bryanloz/DEEPXILINX/MLsuite/overlaybins/1525/xdnn_28_16b.xclbin
  [XBLAS] kernel0: kernelSxdnn_0
  [XDNN] loading xclbin settings from /home/bryanloz/DEEPXILINX/MLsuite/overlaybins/1525/xdnn_28_16b.xclbin.json
  [XDNN] using custom DDR banks 2,2,1,1

  [XDNN] kernel configuration
  [XDNN]   num cores       : 2
  [XDNN]   dsp array width : 28
  [XDNN]   img mem size    : 4 MB
  [XDNN]   version         : 2.1
  [XDNN]   8-bit mode      : 0
  [XDNN]   Max Image W/H   : 0
  [XDNN]   Max Image Depth : 0

  Loading weights/bias/quant_params to FPGA...

  After FPGA (1915.318012 ms)

  After FC (7.054090 ms)

  After Softmax (6.752014 ms)


  ---------- Prediction 0 for dog.jpg ----------
  0.7376 - "n02112018 Pomeranian"
  0.0785 - "n02123394 Persian cat"
  0.0365 - "n02085620 Chihuahua"
  0.0183 - "n02492035 capuchin, ringtail, Cebus capucinus"
  0.0150 - "n02094433 Yorkshire terrier"


  Success!
  ```

