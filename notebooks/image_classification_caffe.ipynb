{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification w/ Caffe Model\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Caffe model for FPGA acceleration  \n",
    "We will prepare a trained Inception v1 model, and then run a single inference.  \n",
    "\n",
    "# Model Preparation (Offline Process, Performed Once):\n",
    "\n",
    "## Phase 1: Compile The Model  \n",
    "    * A Network Graph (prototxt) and a Weights Blob (caffemodel) are compiled\n",
    "    * The network is optimized\n",
    "    * FPGA Instructions are generated\n",
    "      * These instructions are required to run the network in \"one-shot\", and minimize data movement\n",
    "\n",
    "## Phase 2: Quantize The Model\n",
    "    * The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT16 or INT8\n",
    "    * This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve faster inference\n",
    "      * While floating point precision is useful in the model training scenario\n",
    "          It is not required for high speed, high accuracy inference\n",
    "    \n",
    "# Model Deployment (Online Process, Typically Performed Iteratively):  \n",
    "    \n",
    "## Phase 3: Deploy The Model\n",
    "Once you have the outputs of the compiler and quantizer, you will use the xfDNN deployment APIs to:\n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run fully connected layers on the CPU\n",
    "7. Run Softmax on CPU\n",
    "8. Print the result (or send the result for further processing)\n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "First, we will look at compiling, quantizing and deploying a Inception v1 image classification example. After completing the example, we will look at deploying a custom model, using the same steps. \n",
    "\n",
    "### Step 1. Import required packages, check environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,cv2\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Bring in Xilinx ML-Suite Compiler, Quantizer, PyXDNN\n",
    "from xfdnn.tools.compile.bin.xfdnn_compiler_caffe import CaffeFrontend as xfdnnCompiler\n",
    "from xfdnn.tools.quantize.quantize import CaffeFrontend as xfdnnQuantizer\n",
    "import xfdnn.rt.xdnn as pyxfdnn\n",
    "import xfdnn.rt.xdnn_io as pyxfdnn_io\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "print(\"Current working directory: %s\" % os.getcwd())\n",
    "print(\"Running on host: %s\" % os.uname()[1])\n",
    "print(\"Running w/ LD_LIBRARY_PATH: %s\" %  os.environ[\"LD_LIBRARY_PATH\"])\n",
    "print(\"Running w/ XILINX_OPENCL: %s\" %  os.environ[\"XILINX_OPENCL\"])\n",
    "print(\"Running w/ XCLBIN_PATH: %s\" %  os.environ[\"XCLBIN_PATH\"])\n",
    "print(\"Running w/ PYTHONPATH: %s\" %  os.environ[\"PYTHONPATH\"])\n",
    "print(\"Running w/ SDACCEL_INI_PATH: %s\" %  os.environ[\"SDACCEL_INI_PATH\"])\n",
    "\n",
    "id = !whoami",
    "\n",
    "print (\"Running as user: %s\" % id[0])\n",
    "# Make sure there is no error in this cell\n",
    "# The xfDNN runtime depends upon the above environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Use a config dictionary to pass parameters.\n",
    "\n",
    "Here, we will setup and use a config dictionary to simplify handling of the arguments. In this cell, we will also perform some basic error checking. For this first example, we will attempt to classify a picture of a dog. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# Quick check to see if we are running on AWS, Nimbix, if not assume 1525 box\n",
    "if os.path.exists(\"/sys/hypervisor/uuid\"):\n",
    "    with open(\"/sys/hypervisor/uuid\") as fp:\n",
    "        contents = fp.read()\n",
    "        if \"ec2\" in contents:\n",
    "            print(\"Runnning on Amazon AWS EC2\")\n",
    "            config[\"device\"] = \"aws\"\n",
    "elif \"nimbix\" in id[0]:\n",
    "    print(\"Runnning on NIMBIX\")\n",
    "    config[\"device\"] = \"nimbix\"\n",
    "else:\n",
    "    print(\"Runnning on VCU1525\")\n",
    "    config[\"device\"] = \"1525\"\n",
    "\n",
    "config[\"images\"] = [\"../examples/classification/dog.jpg\"] # Image of interest (Must provide as a list)\n",
    "\n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define an xfdnnCompiler instance and pass it arguments.  \n",
    "To simplify handling of arguments, we continue to use a config dictionary. Take a look at the dictionary entries below. \n",
    "\n",
    "The arguments that need to be passed are: \n",
    "- `prototxt` - Caffe representation of the network\n",
    "- `caffemodel` - Pre-trained Model for the network \n",
    "- `outmodel` - Filename for saving the prototxt and caffemodel of the optimized network\n",
    "- `fpgacommands` - Filename to save micro-instruction produced by the compiler needed to deploy\n",
    "- `memory` - Parameter to set the on-chip memory for the target xDNN overlay. This example will target an overlay with 5 MB of cache. \n",
    "- `dsp` - Parameter to set the size of the target xDNN overlay. This example uses an overlay of size 32x56 DSPs. \n",
    "\n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "Memory, and DSP are critical arguments that correspond to the hardware accelerator you plan to load onto the FPGA.  \n",
    "The memory, and dsp parameters can be extracted from the name of the fpga programming file \"xclbin\".  \n",
    "Don't worry about this detail for now. Just know that if you change the xclbin, you have to recheck these parameters.  \n",
    "//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  \n",
    "\n",
    "The xfDNN Compiler interfaces with Caffe to read a network graph, and generates a sequence of instructions for the xfDNN Deploy APIs to execute on the FPGA.  \n",
    "\n",
    "During this process the xfDNN Compiler performs computational graph traversal, node merging and optimization, memory allocation and optimization and, finally, micro-instruction generation.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler Arguments\n",
    "\n",
    "config[\"prototxt\"] = \"../models/caffe/bvlc_googlenet_without_lrn/fp32/bvlc_googlenet_without_lrn_deploy.prototxt\" \n",
    "config[\"caffemodel\"] = \"../models/caffe/bvlc_googlenet_without_lrn/fp32/bvlc_googlenet_without_lrn.caffemodel\"\n",
    "config[\"outmodel\"] = \"work/optimized_model\" # String for naming optimized prototxt, caffemodel\n",
    "config[\"fpgacommands\"] = \"work/fpga.cmds\" # Compiler will generate FPGA instructions\n",
    "config[\"memory\"] = 5 # Available on-chip SRAM\n",
    "config[\"dsp\"] = 56 # Width of Systolic Array\n",
    "\n",
    "compiler = xfdnnCompiler(\n",
    "    networkfile=config[\"prototxt\"],       # Prototxt filename: input file\n",
    "    weights=config[\"caffemodel\"],         # Floating Point Weights: input file\n",
    "    anew=config[\"outmodel\"],              # String for intermediate prototxt/caffemodel\n",
    "    generatefile=config[\"fpgacommands\"],  # Script filename: output file\n",
    "    memory=config[\"memory\"],              # Available on chip SRAM within xclbin\n",
    "    dsp=config[\"dsp\"]                     # Rows in DSP systolic array within xclbin\n",
    ")\n",
    "\n",
    "# Invoke compiler\n",
    "try:\n",
    "    compiler.compile()\n",
    "\n",
    "    # The compiler extracts the floating point weights from the .caffemodel. \n",
    "    # This weights dir will be stored in the work dir with the appendex '_data'. \n",
    "    # The compiler will name it after the caffemodel, and append _data\n",
    "    config[\"datadir\"] = \"work/\" + config[\"caffemodel\"].split(\"/\")[-1]+\"_data\"\n",
    "        \n",
    "    if os.path.exists(config[\"datadir\"]) and os.path.exists(config[\"fpgacommands\"]+\".json\"):\n",
    "        print(\"Compiler successfully generated JSON and the data directory: %s\" % config[\"datadir\"])\n",
    "    else:\n",
    "        print(\"Compiler failed to generate the JSON or data directory: %s\" % config[\"datadir\"])\n",
    "        raise\n",
    "        \n",
    "    print(\"**********\\nCompilation Successful!\\n\")\n",
    "    \n",
    "    import json\n",
    "    data = json.loads(open(config[\"fpgacommands\"]+\".json\").read())\n",
    "    print(\"Network Operations Count: %d\"%data['ops'])\n",
    "    print(\"DDR Transfers (bytes): %d\"%data['moveops']) \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Failed to complete compilation:\",e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Create Quantizer Instance and run it.\n",
    "\n",
    "To simplify handling of arguments, a config dictionary is used. Take a look at the dictionary below.\n",
    "\n",
    "The arguments that need to be passed are:\n",
    "- `deploy_model` - Filename generated by the compiler for the optimized prototxt and caffemodel.\n",
    "- `quantizecfg` - Output JSON filename of quantization scaling parameters. \n",
    "- `bitwidths` - Desired precision from quantizer. This is to set the precision for [image data, weight bitwidth, conv output]. All three values need to be set to the same setting. The valid options are `16` for Int16 and `8` for Int8.  \n",
    "- `in_shape` - Sets the desired input image size of the first layer. Images will be resized to these demensions and must match the network data/placeholder layer.\n",
    "- `transpose` - Images start as H,W,C (H=0,W=1,C=2) transpose swaps to C,H,W (2,0,1) for typical networks.\n",
    "- `channel_swap` - Depending on network training and image read, can swap from RGB (R=0,G=1,B=2) to BGR (2,1,0).\n",
    "- `raw_scale` - Depending on network training, scale pixel values before mean subtraction.\n",
    "- `img_mean` - Depending on network training, subtract image mean if available.\n",
    "- `input_scale` - Depending on network training, scale after subtracting mean.\n",
    "- `calibration_size` - Number of images the quantizer will use to calculate the dynamic range. \n",
    "- `calibration_directory` - Location of dir of images used for the calibration process. \n",
    "\n",
    "Below is an example with all the parameters filled in. `channel_swap` `raw_scale` `img_mean` `input_scale` are image preprocessing arguments specific to a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a config dictionary to pass parameters to the compiler\n",
    "# Quantizer Arguments\n",
    "#config[\"outmodel\"] = Defined in Step 1 # String for naming intermediate prototxt, caffemodel\n",
    "config[\"quantizecfg\"] = \"work/quantization_params.json\" # Quantizer will generate quantization params\n",
    "config[\"bitwidths\"] = [16,16,16] # Supported quantization precision\n",
    "config[\"in_shape\"] = [3,224,224] # Images will be resized to this shape -> Needs to match prototxt\n",
    "config[\"transpose\"] = [2,0,1] # (H,W,C)->(C,H,W) transpose argument to quantizer\n",
    "config[\"channel_swap\"] = [2,1,0] # (R,G,B)->(B,G,R) Channel Swap argument to quantizer\n",
    "config[\"raw_scale\"] = 255.0 # Raw scale of input pixels, i.e. 0 <-> 255\n",
    "config[\"img_mean\"] = [104.007, 116.669, 122.679] # Mean of the training set used for mean subtraction\n",
    "config[\"input_scale\"] = 1.0 # Input multiplier, Images are scaled by this factor after mean subtraction\n",
    "config[\"calibration_size\"] = 15 # Number of calibration images quantizer will use\n",
    "config[\"calibration_directory\"] = \"../xfdnn/tools/quantize/calibration_directory\" # Directory of images\n",
    "\n",
    "quantizer = xfdnnQuantizer(\n",
    "    deploy_model=config[\"outmodel\"]+\".prototxt\",        # Model filename: input file\n",
    "    weights=config[\"outmodel\"]+\".caffemodel\",           # Floating Point weights\n",
    "    output_json=config[\"quantizecfg\"],                    # Quantization JSON output filename\n",
    "    bitwidths=config[\"bitwidths\"],                        # Fixed Point precision: 8,8,8 or 16,16,16\n",
    "    dims=config[\"in_shape\"],                              # Image dimensions [C,H,W]\n",
    "    transpose=config[\"transpose\"],                        # Transpose argument to caffe transformer\n",
    "    channel_swap=config[\"channel_swap\"],                  # Channel swap argument to caffe transfomer\n",
    "    raw_scale=config[\"raw_scale\"],                        # Raw scale argument to caffe transformer\n",
    "    mean_value=config[\"img_mean\"],                        # Image mean per channel to caffe transformer\n",
    "    input_scale=config[\"input_scale\"],                    # Input scale argument to caffe transformer\n",
    "    calibration_size=config[\"calibration_size\"],          # Number of calibration images to use\n",
    "    calibration_directory=config[\"calibration_directory\"] # Directory containing calbration images\n",
    ")\n",
    "\n",
    "# Invoke quantizer\n",
    "try:\n",
    "    quantizer.quantize()\n",
    "\n",
    "    import json\n",
    "    data = json.loads(open(config[\"quantizecfg\"]).read())\n",
    "    print(\"**********\\nSuccessfully produced quantization JSON file for %d layers.\\n\"%len(data['network']))\n",
    "except Exception as e:\n",
    "    print(\"Failed to quantize:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3. Deploy The Model.\n",
    "Next, we need to utilize the xfDNN APIs to deploy our network to the FPGA. We will walk through the deployment APIs, step by step: \n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run fully connected layers on the CPU\n",
    "7. Run Softmax on CPU\n",
    "8. Print the result (or send the result for further processing)\n",
    "9. When you are done, close the handle to the FPGA\n",
    "\n",
    "First, we will create the handle to communicate with the FPGA and choose which FPGA overlay to run the inference on. For this lab, we will use overlay_3. You can learn about other overlay options in the ML Suite Tutorials [here][].  \n",
    "\n",
    "[here]: https://github.com/Xilinx/ml-suite\n",
    "        \n",
    "### Step 5. Open a handle for FPGA communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handle with which to communicate to the FPGA\n",
    "# The actual handle is managed by pyxfdnn\n",
    "\n",
    "config[\"xclbin\"] = \"../overlaybins/\" + config[\"device\"] + \"/overlay_3.xclbin\" # Chosen Hardware Overlay\n",
    "## NOTE: If you change the xclbin, we likely need to change some arguments provided to the compiler\n",
    "## Specifically, the DSP array width, and the memory arguments\n",
    "\n",
    "config[\"xfdnn_library\"] = \"../xfdnn/rt/xdnn_cpp/lib/libxfdnn.so\" # Library functions called by pyXFDNN\n",
    "\n",
    "ret = pyxfdnn.createHandle(config['xclbin'], \"kernelSxdnn_0\", config['xfdnn_library'])\n",
    "if ret:                                                             \n",
    "    print(\"ERROR: Unable to create handle to FPGA\")\n",
    "else:\n",
    "    print(\"INFO: Successfully created handle to FPGA\")\n",
    "    \n",
    "# If this step fails, most likely the FPGA is locked by another user, or there is some setup problem with the hardware\n",
    "# For instance the xclbin could have been built for the wrong DSA/shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Apply quantization scaling and transfer model weights to the FPGA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize, and transfer the weights to FPGA DDR\n",
    "\n",
    "# config[\"datadir\"] = \"work/\" + config[\"caffemodel\"].split(\"/\")[-1]+\"_data\" # From Compiler\n",
    "config[\"scaleA\"] = 10000 # Global scaler for weights (Must be defined, although not used)\n",
    "config[\"scaleB\"] = 30 # Global scaler for bias (Must be defined, although not used)\n",
    "config[\"PE\"] = 0 # Run on Processing Element 0 - Different xclbins have a different number of Elements\n",
    "\n",
    "(weightsBlob, fcWeight, fcBias ) = pyxfdnn_io.loadWeights(config)\n",
    "\n",
    "# Note that this function returns pointers to weights corresponding to the layers that will be implemented in the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Allocate space in host memory for inputs, load images from disk, and prepare images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for inputs, Load images from disk\n",
    "\n",
    "config[\"transform\"] = \"resize\" # Resize Images to fit into network\n",
    "config[\"firstfpgalayer\"] = \"conv1/7x7_s2\" # Name of first layer to be ran on the FPGA -> Needs to match prototxt\n",
    "\n",
    "(fpgaInputs, batch_sz) = pyxfdnn_io.prepareInput(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Allocate space in host memory for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate space in host memory for outputs\n",
    "\n",
    "config[\"fpgaoutsz\"] = 1024 # Number of elements in the activation of the last layer ran on the FPGA\n",
    "\n",
    "fpgaOutputs = pyxfdnn_io.prepareOutput(config['fpgaoutsz'], batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Write optimized micro-code to the xDNN Processing Engine on the FPGA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write FPGA Instructions to FPGA and Execute the network!\n",
    "if len(pyxfdnn._xdnnManager._handles) > 0: # Just make sure FPGA still available\n",
    "    pyxfdnn.execute(\n",
    "        config[\"fpgacommands\"],\n",
    "        weightsBlob,\n",
    "        fpgaInputs,\n",
    "        fpgaOutputs,\n",
    "        batch_sz, # Number of images we are processing\n",
    "        config['quantizecfg'], \n",
    "        config['scaleB'], \n",
    "        config['PE']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Execute the Fully Connected Layers on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"outsz\"] = 1000 # Number of elements output by FC layers\n",
    "config[\"useblas\"] = True # Accelerate Fully Connected Layers in the CPU\n",
    "\n",
    "if len(pyxfdnn._xdnnManager._handles) > 0: # Just make sure FPGA still available\n",
    "    fcOut = pyxfdnn.computeFC(\n",
    "        fcWeight, \n",
    "        fcBias, \n",
    "        fpgaOutputs,\n",
    "        batch_sz, \n",
    "        config['outsz'], \n",
    "        config['fpgaoutsz'], \n",
    "        config['useblas'] # Can use cblas if True or numpy if False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Execute the Softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the softmax to convert the output to a vector of probabilities\n",
    "softmaxOut = pyxfdnn.computeSoftmax(fcOut, batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12. Output the classification prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification given the labels synset_words.txt (Imagenet classes)\n",
    "\n",
    "config[\"labels\"] = \"../examples/classification/synset_words.txt\"\n",
    "pyxfdnn_io.printClassification(softmaxOut, config);\n",
    "\n",
    "#Print Original Image for Reference \n",
    "img = cv2.imread(config[\"images\"][0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.title(config[\"images\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13. Close the handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyxfdnn.closeHandle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14. Your Turn! \n",
    "Great work! Now it is your turn! \n",
    "\n",
    "We have another trained model which leverages the Inception v1 architecture.    \n",
    "This one is trained on the flowers dataset which has 102 classes.  \n",
    "\n",
    "The final, fully connected layer has only 102 outputs for 102 output categories.  \n",
    "\n",
    "This means that the graph and weights are different.\n",
    "\n",
    "Update this notebook to classify pretty flowers instead!\n",
    "\n",
    "Start by clicking **Kernel** from the menu, and then select **Reset & Clear Output**. \n",
    "\n",
    "Next update the parameters in the following steps:   \n",
    "\n",
    "### In Step 2:\n",
    "Set `config[\"images\"]` to a new image.  A test flower image is located here: `\"../examples/classification/flower.jpg\"`\n",
    "\n",
    "### In Step 3:\n",
    "Set `config[\"prototxt\"]` to a new network: To classify flowers, use the prototxt located here: `\"../models/caffe/flowers102/fp32/bvlc_googlenet_without_lrn_deploy.prototxt\"`   \n",
    "\n",
    "Set `config[\"caffemodel\"]` to a model trained to classify flowers. The flowers caffe model is located here: `\"../models/caffe/flowers102/fp32/bvlc_googlenet_without_lrn.caffemodel\"`\n",
    "\n",
    "### In Step 10:\n",
    "Set `config[\"outsz\"]` to reflect the number of classification categories for flowers, which is `102`.  \n",
    "\n",
    "### In Step 12:\n",
    "Set `config[\"labels\"]` with the flower labels. The labels are located here: `\"../models/caffe/flowers102/data/synset_words.txt\"`  \n",
    "\n",
    "When you have made all the updates, click **Kernel** from the menu, and select **Restart & Run All** to see if you can classify the flower! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
