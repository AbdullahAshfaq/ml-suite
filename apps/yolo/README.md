# YOLOv2 Object Detection Tutorial

## Introduction
You only look once (YOLO) is a state-of-the-art, real-time object detection algorithm.  
The algorithm was published by Redmon et al. in 2016 via the following publications:
[YOLOv1](https://arxiv.org/abs/1506.02640),
[YOLOv2](https://arxiv.org/abs/1612.08242).  
The same author has already released YOLOv3, and some experimental tiny YOLO networks. This tutorial focuses on YOLOv2.
This application requires more than just simple classification. The task here is to detect the presence of objects, and localize them within a frame. 
Please refer to the papers for full algorithm details, and/or watch [this.](https://www.youtube.com/watch?v=9s_FpMpdYW8). 
In this tutorial, the network was trained on the 80 class [COCO dataset.](http://cocodataset.org/#home)

## Background
The authors of the YOLO papers used their own programming framework called "Darknet" for research, and development. The framework is written in C, and was [open sourced.](https://github.com/pjreddie/darknet) Additionally, they host documentation, and pretrained weights [here.](https://pjreddie.com/darknet/yolov2/) Currently, the Darknet framework is not supported by Xilinx's ML Suite. Additionally, there are some aspects of the YOLOv2 network that are not supported by the Hardware Accelerator, such as the leaky ReLU activation function. For these reasons the network was modified, retrained, and converted to caffe. In this tutorial we will run the network accelerated on an FPGA using 16b quantized weights and a hardware kernel implementing a 56x32 systolic array with 5MB of image RAM. All convolutions/pools are accelerated on the FPGA fabric, while the final sigmoid, softmax, and non-max suppression functions are executed on the CPU. Converting from Darknet to Caffe will be discussed in future documentation.

### Network Modifications
* Leaky ReLU replaced by ReLU
* "reorg" layer a.k.a. "space_to_depth" layer replaced by MAX POOL
 
## Running the Application

Xilinx has provided a demo application showing how YOLOv2 can be ran "end to end", meaning we will run all of the required offline, and online software to get some example results.   
[yolo.py](./yolo.py) is the top level python module where you will see how the compiler, quantizer, and xyolo module are invoked.   
[configs.py](./configs.py) is a configuration file used to modify the desired run configuration. We are supporting 608x608/224x224 images, 16b/8b quantization.  
[xyolo.py](./xyolo.py) is a python class meant to be reusable, but it also demonstrates how to use the PYXDNN in a custom application. It provides a YOLO detect method.  
[run.sh](./run.sh) is a bash script used to run some enviornment setup, and launch the demo app  

 To run:
 1. Connect to F1 or Local Hardware
 
 2. Download the xilinx trained models from Xilinx.com, save as models at the root of this repo 
 
 3. `cd MLsuite/apps/yolo`
 
 4. Familiarize yourself with the script usage by:  
  `./run.sh -h`  
  The key parameters are:
    - -p `platform` Valid values are `alveo-u200`, `alveo-u250`, `aws`, `nimbix`, `1525`, `1525-ml` and `gpu` 
    - -t `test` - Currently valid value are  `test_detect` , `streaming_detect` and `streaming_detect_benchmark'
    - -k `kernel config` - Valid values are `large` or `v3` - Used to select overlaybins
    - -b `quantization precision` - Valid values are `16` or `8` - corresponding to INT16 or INT8
      - Note: XDNNv3 only supports 8 which is also the default precision (INT8)
    - -g `calclulate mAP score` - specify the folder containing the labels(txt) files for set of images given with -d option
    - -d `images` - Directory of images
    - -cw 'net weights dumped by complier` - specify folder containing weights and biases as txt files
    - -cn 'net.cmds.json file generated by compler` - specify schedule generated by compiler
    - -cq 'net_quant.json file generated by quantizer` - specify quantizer parameters generated by quantizer
    
    
      
## Example Invocations
1. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x224x224, with XDNNv3:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k v3 -b 8 -m yolo_v2_224
    ```
2. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x224x224, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_224
    ```
3. Object detection on a set of images on alveo-u200, with yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_608
    ```
4. Object detection  compute mAP score on a set of images on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t test_detect -k large -b 8 -m yolo_v2_tiny_608 -g labels/val2014/ -d val2014/
    ```
5. Object detection in streaming mode on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t streaming_detect -k large -b 8 -m yolo_v2_tiny_608 -cw yolo_v2_tiny.caffemodel_data/ -cn yolo_v2_tiny.cmds.json -cq yolo_v2_8bit_deploy.json
    ```
6. Object detection in benchmarking mode on alveo-u200, with tiny yolo-v2 input 3x608x608, with 8-bit XDNNv2 large kernel:
    ```sh
    $ ./run.sh -p alveo-u200 -t streaming_detect_benchmark -k large -b 8 -m yolo_v2_tiny_608 -cw yolo_v2_tiny.caffemodel_data/ -cn yolo_v2_tiny.cmds.json -cq yolo_v2_8bit_deploy.json
    ```
7. Object detection on gpu, with tiny yolo-v2 input 3x608x608, with 8-bit precison:
    ```sh
    $ ./run.sh -p gpu -t test_detect -m yolo_v2_tiny_608 
    ```


Refer to the Using Anaconda on AWS instructions located [here][]. 

 Upon success, you will see several bounding box predictions printed for the images in the 'out' directory

Note: After the first initial run, it is possible to run the demo with `python yolo.py` the run.sh script is setting up some key env variables, and building the non-max suppression binary. However, that only needs to be done once, in a shell

[here]: ../../docs/tutorials/start-anaconda.md
